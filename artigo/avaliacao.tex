\chapter{Análise de Trabalhos Relacionados}
\label{ch:analise}

NIDS baseado em anomalia tem se mostrado uma área de estudo promissora, mas ainda possui grande margem para pesquisa e melhorias.
A fim de analisar o estado da arte, considerei o estudo feito em \cite{tsai09}, que traz dados de 55 artigos publicados
entre os anos 2000 e 2007. A partir dessa análise, pude extrair algumas inferências sobre as diferentes técnicas utilizadas.
\begin{itemize}
    \item  26 se baseiam no uso de classificadores unitários, apesar de que o uso de
    classificadores híbridos tem aumentado durante os últimos anos.
    Classificadores compostos são usados por poucos, totalizando apenas 6.

    \item Os algoritmos unitários mais estudados para detecção de instrusão foram SVM (7) e K-NN (6).

    \item Quanto a classificadores híbridos, o método \textit{integrado} é o mais popular, consistindo de 10 dos 23.

    \item A utilização de seleção de atributos tem crescido com o tempo. Enquanto em 2003 apenas um quarto dos estudos analisados
por \cite{tsai09} utilizava tal técnica, em 2007 foi o caso de 9 dentre 11. Ou seja, apenas dois (\cite{peddabachigari07}, \cite{li07}) não utilizaram.

    \item Como há poucas bases de dados públicas para detecção de instrusão, a maioria dos estudos se baseia nas
    bases KDD Cup 99 (30) e DARPA 1998 (18).
\end{itemize}

\par Quanto aos dados, as técnicas pioneiras costumavam criar modelos considerando todo o corpo do pacote. Entretanto,
eles separavam as instâncias com base no tamanho do pacote e portas de destino, diminuindo o contexto de cada modelo.
Técnicas posteriores levam em consideração partes específicas do conteúdo do pacote. Aparentemente, anomalias mais
sutis podem ser detectadas a uma taxa menor de Falso Positivo quando usando um contexto mais específico.
\par Dois artigos recentes trouxeram implementações promissoras a respeito de sistemas de detecção de
anomalia. Um deles (\cite{lin12}) apresenta um algoritmo inteligente e o outro (\cite{papadonikolakis12}) mostra o
ganho de desempenho ao se programar o sistema diretamente em uma placa FPGA. Para os fins desta monografia, não irei
implementar o algoritmo em FPGA e, apesar de o foco não ser classificadores em cascata, os estudos relacionados merecem
notoriedade por consituirem o estado da arte no que se propõem a fazer.

\section{Processando grandes quantidades de dados}
A simulação apresentada em \cite{lin12} usou a base do KDD Cup 99. O algoritmo funciona pré-processando os dados de treino e teste e gerando uma solução
inicial aleatória. A solução é atualizada a cada iteração do algoritmo. Enquanto um limite não é satisfeito,
são utilizados Support Vector Machine (SVM) e Simulated Annealing (SA) para selecionar o melhor conjunto de
características. Por conseguinte, Árvore de Decisão e SA são usados para aumentar a precisão do teste e construir
regras de decisão. No final, a melhor precisão e as melhores características e regras de decisão são anunciadas.
\par Nos testes descritos no  artigo, após a seleção de atributos, foram utilizadas 23 características durante o processo de
classificação. O algoritmo conseguiu atingir uma precisão de 99,6\%, o que representa um grande ganho quando comparado
 a outros resultados até o momento.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Aumentado a velocidade de processamento}
Enquanto o algoritmo anterior pretende otimizar apenas a precisão, o estudo apresentado em \cite{papadonikolakis12}
foca em na aceleração do processo de classificação. Aqui também foi utilizado SVM para implementar o A-NIDS, mas dessa
vez diretamente em um FPGA, fazendo uso de seu potencial de processamento paralelo.
\par Foram propostas três arquiteturas diferentes.
\begin{enumerate}
    \item \textit{Heterogeneous Baseline Classifier} -- O objetivo é explorar os requerimentos de precisão dos atributos. Ele
    tenta maximizar paralelização enquanto mantém a relação entre blocos de processamneto digital e blocos lógicos.
    \item \textit{Fit Cascade Chain} -- Aproveita o potencial de aritmética customizada oferecido pelo FPGA para criar uma
    cascata com duas precisões aritméticas diferentes. Primeiro um classificador de baixo custo e alta vazão com baixa
    precisão é usado. Somente os dados que não puderem ser classificados são alimentados ao segundo classificador. Este
    possui alta precisão e, logo, maior custo, mas com menos requisições de vazão.
    \item \textit{Reconfigurable Cascade Chain} -- Foi projetado para problemas com grandes quantidades de dados, que podem
    exceder as limitações de hardware. Essa arquitetura propõe uma reconfiguração do FPGA depois do estágio de treino e
    expande o espaço de design possível. Isso permite a utilização de mais blocos durante as classificação.
\end{enumerate}
\par A base utilizada para testes foi a \textit{MINIST Dataset}. Comparando com testes prévios em GPU, o design
\textit{Heterogeneous Baseline Classifier} processou os dados 7 vezes mais rápido. Os melhores resultados foram
obtidos usando \textit{Reconfigurable Cascade Chain}, com um ganho de 20 vezes, isto é, levando apenas 5\% do tempo
que a GPU consumiu.